{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOQXXOO02WlN",
        "outputId": "ade05be3-7ee0-46c0-d9c5-e964253a47f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Evaluating answers: 100%|██████████| 271/271 [06:36<00:00,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Factual consistency results saved to: rag_responses_vanilla_evaluation_data_FACTCC.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load public NLI-based factual consistency model\n",
        "model_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Label mapping: 0 = contradiction, 1 = neutral, 2 = entailment\n",
        "labels = [\"Contradiction\", \"Neutral\", \"Entailment\"]\n",
        "\n",
        "# === Load and Save Functions ===\n",
        "def load_json(path):\n",
        "    with open(path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def save_json(data, path):\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "# === Evaluation Function ===\n",
        "def evaluate_factual_consistency(data):\n",
        "    results = []\n",
        "    for item in tqdm(data, desc=\"Evaluating answers\"):\n",
        "        hypothesis = item.get(\"answer\", \"\").strip()\n",
        "        premise = item.get(\"ground_truth\", \"\").strip()\n",
        "\n",
        "        if not hypothesis or not premise:\n",
        "            item[\"factcc_label\"] = \"Skipped\"\n",
        "            item[\"factcc_confidence\"] = 0.0\n",
        "            results.append(item)\n",
        "            continue\n",
        "\n",
        "        inputs = tokenizer(hypothesis, premise, return_tensors=\"pt\", truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "        pred_label_idx = torch.argmax(probs).item()\n",
        "        predicted_label = labels[pred_label_idx]\n",
        "        confidence = round(probs[0, pred_label_idx].item(), 4)\n",
        "\n",
        "        item[\"factcc_label\"] = \"Entailed\" if predicted_label == \"Entailment\" else \"Not Entailed\"\n",
        "        item[\"factcc_confidence\"] = confidence\n",
        "        results.append(item)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "#  input/output file paths\n",
        "input_path = \"rag_responses_vanilla_evaluation_data.json\"\n",
        "output_path = \"rag_responses_vanilla_evaluation_data_FACTCC.json\"\n",
        "\n",
        "data = load_json(input_path)\n",
        "evaluated_data = evaluate_factual_consistency(data)\n",
        "save_json(evaluated_data, output_path)\n",
        "\n",
        "print(f\"✅ Factual consistency results saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "with open('rag_responses_vanilla_evaluation_data_FACTCC.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "labels = [item['factcc_label'] for item in data]\n",
        "confidences = [item['factcc_confidence'] for item in data]\n",
        "\n",
        "# Get counts\n",
        "counter = Counter(labels)\n",
        "total = sum(counter[label] for label in [\"Entailed\", \"Not Entailed\"])\n",
        "fc_rate = counter[\"Entailed\"] / total if total > 0 else 0\n",
        "\n",
        "# Average confidences\n",
        "def avg_conf(label):\n",
        "    return np.mean([item['factcc_confidence'] for item in data if item['factcc_label'] == label]) if counter[label] else 0\n",
        "\n",
        "print(\"Total samples:\", len(data))\n",
        "print(\"Entailed:\", counter[\"Entailed\"])\n",
        "print(\"Not Entailed:\", counter[\"Not Entailed\"])\n",
        "print(\"Skipped:\", counter[\"Skipped\"])\n",
        "print(\"Factual Consistency Rate: {:.2%}\".format(fc_rate))\n",
        "print(\"Avg Conf Entailed: {:.3f}\".format(avg_conf(\"Entailed\")))\n",
        "print(\"Avg Conf Not Entailed: {:.3f}\".format(avg_conf(\"Not Entailed\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04WE_XRhrZ3L",
        "outputId": "68f05e0e-a35b-43b2-f1e5-b36973d0d0f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 271\n",
            "Entailed: 54\n",
            "Not Entailed: 217\n",
            "Skipped: 0\n",
            "Factual Consistency Rate: 19.93%\n",
            "Avg Conf Entailed: 0.922\n",
            "Avg Conf Not Entailed: 0.953\n"
          ]
        }
      ]
    }
  ]
}